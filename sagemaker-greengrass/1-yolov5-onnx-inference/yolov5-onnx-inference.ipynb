{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YoloV5 ONNX inference example\n",
    "\n",
    "Can reference the [link](https://github.com/ultralytics/yolov5/issues/343#issuecomment-658021043). It's a really interesting discussion thread about YoloV5 ONNX detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime     # CPU build\n",
    "# pip install onnxruntime-gpu   # GPU build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 1791, done.\u001b[K\n",
      "remote: Total 1791 (delta 0), reused 0 (delta 0), pack-reused 1791\u001b[K\n",
      "Receiving objects: 100% (1791/1791), 5.06 MiB | 39.23 MiB/s, done.\n",
      "Resolving deltas: 100% (1165/1165), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime, os, sys\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/myAWSStudyBlog/sagemaker-greengrass/1-yolov5-onnx-inference/yolov5'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_yolov5 = os.path.abspath(os.getcwd()) + \"/yolov5\"\n",
    "fp_yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python36.zip',\n",
       " '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6',\n",
       " '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/lib-dynload',\n",
       " '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages',\n",
       " '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/home/ec2-user/.ipython',\n",
       " '/home/ec2-user/SageMaker/myAWSStudyBlog/sagemaker-greengrass/1-yolov5-onnx-inference/yolov5']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append(fp_yolov5)\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_name:images output_name:output\n"
     ]
    }
   ],
   "source": [
    "fp_onnx = './yolov5s.onnx'\n",
    "session = onnxruntime.InferenceSession(fp_onnx)\n",
    "session.get_modelmeta()\n",
    "first_input_name = session.get_inputs()[0].name\n",
    "first_output_name = session.get_outputs()[0].name\n",
    "\n",
    "print(f'input_name:{first_input_name} output_name:{first_output_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_onnx(official=True, image_path=None):\n",
    "    num_classes = 80\n",
    "    anchors = [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]  # 5s\n",
    "\n",
    "    session = onnxruntime.InferenceSession(fp_onnx)\n",
    "    # print(\"The model expects input shape: \", session.get_inputs()[0].shape)\n",
    "    batch_size = session.get_inputs()[0].shape[0]\n",
    "    img_size_h = session.get_inputs()[0].shape[2]\n",
    "    img_size_w = session.get_inputs()[0].shape[3]\n",
    "\n",
    "    # input\n",
    "    image_src = Image.open(image_path)\n",
    "    # resized = letterbox_image(image_src, (img_size_w, img_size_h))\n",
    "    # convert image to numpy array\n",
    "    np_image = asarray(image_src)\n",
    "    print(np_image.shape)\n",
    "    \n",
    "    resized = letterbox(np_image, (img_size_w, img_size_h))\n",
    "\n",
    "    print(resized.shape)    \n",
    "    img_in = np.transpose(resized, (2, 0, 1)).astype(np.float32)  # HWC -> CHW\n",
    "    img_in = np.expand_dims(img_in, axis=0)\n",
    "    img_in /= 255.0\n",
    "    # print(\"Shape of the image input shape: \", img_in.shape)\n",
    "\n",
    "    # inference\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    outputs = session.run(None, {input_name: img_in})\n",
    "\n",
    "    batch_detections = []\n",
    "    if official and len(outputs) == 4:   # model.model[-1].export = boolean ---> True:3 False:4\n",
    "        # model.model[-1].export = False ---> outputs[0] (1, xxxx, 85)\n",
    "        # official\n",
    "        batch_detections = torch.from_numpy(np.array(outputs[0]))\n",
    "        batch_detections = non_max_suppression(batch_detections, conf_thres=0.4, iou_thres=0.5, agnostic=False)\n",
    "    else:\n",
    "        # model.model[-1].export = False ---> outputs[1]/outputs[2]/outputs[2]\n",
    "        # model.model[-1].export = True  ---> outputs\n",
    "        # (1, 3, 20, 20, 85)\n",
    "        # (1, 3, 40, 40, 85)\n",
    "        # (1, 3, 80, 80, 85)\n",
    "        # myself (from yolo.py Detect)\n",
    "        boxs = []\n",
    "        a = torch.tensor(anchors).float().view(3, -1, 2)\n",
    "        anchor_grid = a.clone().view(3, 1, -1, 1, 1, 2)\n",
    "        if len(outputs) == 4:\n",
    "            outputs = [outputs[1], outputs[2], outputs[3]]\n",
    "        for index, out in enumerate(outputs):\n",
    "            out = torch.from_numpy(out)\n",
    "            batch = out.shape[1]\n",
    "            feature_w = out.shape[2]\n",
    "            feature_h = out.shape[3]\n",
    "\n",
    "            # Feature map corresponds to the original image zoom factor\n",
    "            stride_w = int(img_size_w / feature_w)\n",
    "            stride_h = int(img_size_h / feature_h)\n",
    "\n",
    "            conf = out[..., 4]\n",
    "            pred_cls = out[..., 5:]\n",
    "\n",
    "            grid_x, grid_y = np.meshgrid(np.arange(feature_w), np.arange(feature_h))\n",
    "\n",
    "            # cx, cy, w, h\n",
    "            pred_boxes = torch.FloatTensor(out[..., :4].shape)\n",
    "            pred_boxes[..., 0] = (torch.sigmoid(out[..., 0]) * 2.0 - 0.5 + grid_x) * stride_w  # cx\n",
    "            pred_boxes[..., 1] = (torch.sigmoid(out[..., 1]) * 2.0 - 0.5 + grid_y) * stride_h  # cy\n",
    "            pred_boxes[..., 2:4] = (torch.sigmoid(out[..., 2:4]) * 2) ** 2 * anchor_grid[index]  # wh\n",
    "\n",
    "            conf = torch.sigmoid(conf)\n",
    "            pred_cls = torch.sigmoid(pred_cls)\n",
    "\n",
    "            output = torch.cat((pred_boxes.view(batch_size, -1, 4),\n",
    "                                conf.view(batch_size, -1, 1),\n",
    "                                pred_cls.view(batch_size, -1, num_classes)),\n",
    "                               -1)\n",
    "            boxs.append(output)\n",
    "\n",
    "        outputx = torch.cat(boxs, 1)\n",
    "        # NMS\n",
    "        batch_detections = w_non_max_suppression(outputx, num_classes, conf_thres=0.4, nms_thres=0.3)\n",
    "\n",
    "    return batch_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c106c0690298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfp_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp_yolov5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/inference/images/zidane.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofficial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfp_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-991a54296e7e>\u001b[0m in \u001b[0;36mdetect_onnx\u001b[0;34m(official, image_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mresized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mletterbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_size_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mimg_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# HWC -> CHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mimg_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mimg_in\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \"\"\"\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "fp_img = fp_yolov5 + '/inference/images/zidane.jpg'\n",
    "response = detect_onnx(official=True, image_path=fp_img)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
