{"paragraphs":[{"text":"%spark\n\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.PCA\nimport org.apache.spark.sql.SparkSession\nimport com.amazonaws.services.sagemaker.sparksdk.IAMRole\nimport com.amazonaws.services.sagemaker.sparksdk.algorithms\nimport com.amazonaws.services.sagemaker.sparksdk.algorithms.KMeansSageMakerEstimator\nimport com.amazonaws.services.sagemaker.sparksdk.transformation.serializers.ProtobufRequestRowSerializer\nimport com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder\nimport com.amazonaws.services.securitytoken.model.GetCallerIdentityRequest\n\nval spark = SparkSession.builder.getOrCreate\n\n// load mnist data as a dataframe from libsvm \nval region = \"us-east-1\"\nval trainingData = spark.read.format(\"libsvm\")\n  .option(\"numFeatures\", \"784\")\n  .load(s\"s3://sagemaker-sample-data-$region/spark/mnist/train/\")\nval testData = spark.read.format(\"libsvm\")\n  .option(\"numFeatures\", \"784\")\n  .load(s\"s3://sagemaker-sample-data-$region/spark/mnist/test/\")\n\n// substitute your SageMaker IAM role here\nval accountId = AWSSecurityTokenServiceClientBuilder.standard().build()\n    .getCallerIdentity(new GetCallerIdentityRequest()).getAccount()\n\nval roleName = \"SageMakerEMR\"\n\nval roleArn = s\"arn:aws:iam::$accountId:role/$roleName\"\n\n\nval pcaEstimator = new PCA()\n  .setInputCol(\"features\")\n  .setOutputCol(\"projectedFeatures\")\n  .setK(50)\n\nval kMeansSageMakerEstimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  requestRowSerializer =\n    new ProtobufRequestRowSerializer(featuresColumnName = \"projectedFeatures\"),\n  trainingSparkDataFormatOptions = Map(\"featuresColumnName\" -> \"projectedFeatures\"),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(50)\n\nval pipeline = new Pipeline().setStages(Array(pcaEstimator, kMeansSageMakerEstimator))\n\n// train\nval pipelineModel = pipeline.fit(trainingData)\n\nval transformedData = pipelineModel.transform(testData)\ntransformedData.show()","user":"anonymous","dateUpdated":"2019-07-25T11:22:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-24.ec2.internal:4040/jobs/job?id=12","http://ip-172-31-42-24.ec2.internal:4040/jobs/job?id=13","http://ip-172-31-42-24.ec2.internal:4040/jobs/job?id=14","http://ip-172-31-42-24.ec2.internal:4040/jobs/job?id=15","http://ip-172-31-42-24.ec2.internal:4040/jobs/job?id=16"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1564053153415_1801494301","id":"20190725-111233_1525386065","dateCreated":"2019-07-25T11:12:33+0000","dateStarted":"2019-07-25T11:22:11+0000","dateFinished":"2019-07-25T11:19:33+0000","status":"RUNNING","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8141","errorMessage":""},{"text":"%spark\n","user":"anonymous","dateUpdated":"2019-07-25T11:13:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1564053201047_490729510","id":"20190725-111321_2043141051","dateCreated":"2019-07-25T11:13:21+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:8142"}],"name":"spark-pipeline","id":"2EG8K4SUS","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}